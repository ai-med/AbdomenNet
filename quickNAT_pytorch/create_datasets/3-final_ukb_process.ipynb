{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataset in global_vars.py to UKB.\n",
    "from global_vars import *\n",
    "from commons import *\n",
    "\n",
    "import glob \n",
    "import os\n",
    "\n",
    "one_time_n4_optimization = True\n",
    "vol_to_check_list = None\n",
    "exclude = []\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ukb_file_paths(load_from_txt_file=True):\n",
    "    volumes_to_use = []\n",
    "    if load_from_txt_file:\n",
    "        with open(volume_txt_file) as file_handle:\n",
    "                volumes_to_use = file_handle.read().splitlines()\n",
    "    else:\n",
    "        volumes_to_use = [name for name in os.listdir(data_dir)[:5]]\n",
    "\n",
    "    file_paths = {}\n",
    "    \n",
    "    for vol in volumes_to_use[:5]:\n",
    "        if (vol_to_check_list is not None and vol not in vol_to_check_list) or (vol == \"\") or (vol in exclude):\n",
    "            continue\n",
    "            \n",
    "        opp_paths = glob.glob(f'{data_dir}/{vol}/**opp**_[17s, 17sa,17sb]**.nii.gz')\n",
    "        in_paths = glob.glob(f'{data_dir}/{vol}/**in**_[17s,17sa,17sb]**.nii.gz')\n",
    "        f_paths = glob.glob(f'{data_dir}/{vol}/**F**_[17s, 17sa,17sb]**.nii.gz')\n",
    "        w_paths = glob.glob(f'{data_dir}/{vol}/**W**_[17s, 17sa,17sb]**.nii.gz')\n",
    "        \n",
    "        labelmap_paths = glob.glob(f'{label_dir}/{vol}/**')\n",
    "        \n",
    "        vol_madals_paths = dict(\n",
    "        OPP=opp_paths,\n",
    "        IN=in_paths,\n",
    "        F=f_paths,\n",
    "        W=w_paths\n",
    "        )\n",
    "        file_paths[str(vol)]=dict(\n",
    "            VOLUME_PATHS=vol_madals_paths,\n",
    "            LABEL_PATHS=labelmap_paths,\n",
    "        )\n",
    "    return file_paths\n",
    "\n",
    "file_paths = load_ukb_file_paths(True)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vol in file_paths.keys():\n",
    "#     for idx in range(4):\n",
    "#         try:\n",
    "#             path = file_paths[vol]['VOLUME_PATHS']['IN'][idx]\n",
    "#             print(np.min(nb.load(path).get_fdata()), \"IN\", vol)\n",
    "#             path = file_paths[vol]['VOLUME_PATHS']['OPP'][idx]\n",
    "#             print(np.min(nb.load(path).get_fdata()), \"OPP\", vol)\n",
    "#             path = file_paths[vol]['VOLUME_PATHS']['F'][idx]\n",
    "#             print(np.min(nb.load(path).get_fdata()), \"F\", vol)\n",
    "#             path = file_paths[vol]['VOLUME_PATHS']['W'][idx]\n",
    "#             print(np.min(nb.load(path).get_fdata()), \"W\", vol)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual RESCALING.\n",
    "n4_dict = {}\n",
    "for vol in file_paths.keys():\n",
    "    if one_time_n4_optimization:\n",
    "        break\n",
    "    if vol_to_check_list is not None and vol not in vol_to_check_list:\n",
    "        continue\n",
    "    print(f'n4 processing part-1 started with {vol}...')\n",
    "    n4_dict[vol] = []\n",
    "    vol_parts = [[file, read_ras(file)] for file in file_paths[vol]['VOLUME_PATHS']['IN']]\n",
    "    for orig_file, in_image in vol_parts:\n",
    "        n4_dict[vol].append(rescale(in_image, vol, orig_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITK does ot work due to differences in pixel resolution of IN and corresponding OPP Scan.\n",
    "# Only applying once at the end.\n",
    "for vol in file_paths.keys():\n",
    "    if one_time_n4_optimization:\n",
    "        break\n",
    "    if vol_to_check_list is not None and vol not in vol_to_check_list:\n",
    "        continue\n",
    "    print(f'n4-biasfield-correction starting with {vol}...')\n",
    "    for idx, n4_d in enumerate(n4_dict[vol]):\n",
    "        in_file = n4_d['SCALED']\n",
    "        opp_file = file_paths[vol]['VOLUME_PATHS']['OPP'][idx]\n",
    "        new_filename = opp_file.split('/')[-1].split('.')[0]\n",
    "        output_file = f'{n4_corrected_data_dir}/vol/{vol}/{new_filename}_n4_corrected_sitk.nii.gz'\n",
    "        SITK_N4_normalization(in_file, opp_file, output_file)\n",
    "        n4_dict[vol][idx]['OPP_CORRECTED'] = output_file\n",
    "\n",
    "    file_paths[vol]['N4_1'] = n4_dict[vol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STITCHING VOL PARTS HERE\n",
    "for vol in file_paths.keys():\n",
    "    try:\n",
    "        print(f'started with {vol}...')\n",
    "        if vol_to_check_list is not None and vol not in vol_to_check_list:\n",
    "            continue\n",
    "        create_if_not(f'{n4_corrected_data_dir}/vol/{vol}')\n",
    "        file_paths[vol]['ONE'] = {}\n",
    "        for modality_key in file_paths[vol]['VOLUME_PATHS'].keys():\n",
    "            print(f\"processing {modality_key}\")\n",
    "            orig_modal_key = modality_key\n",
    "            if one_time_n4_optimization:\n",
    "                vol_parts = [read_ras(file) for file in file_paths[vol]['VOLUME_PATHS'][modality_key]]\n",
    "            else:\n",
    "                if modality_key == 'OPP':\n",
    "                    vol_parts = [read_ras(data_dict['OPP_CORRECTED']) for data_dict in file_paths[vol]['N4_1']]\n",
    "                    modality_key = modality_key+'_n4_corrected'\n",
    "                else:\n",
    "                    vol_parts = [read_ras(file) for file in file_paths[vol]['VOLUME_PATHS'][modality_key]]\n",
    "\n",
    "#             _ = [print(np.min(vp.get_fdata())) for vp in vol_parts]\n",
    "            ras_stitched = multi_vol_stitching(vol_parts, sampling=SAMPLING)\n",
    "#             print(np.min(ras_stitched.get_fdata()))\n",
    "            save_volume(ras_stitched, f'{n4_corrected_data_dir}/vol/{vol}/{modality_key}_ras_stitched')\n",
    "            file_paths[vol]['ONE'][f'{orig_modal_key}'] = f'{n4_corrected_data_dir}/vol/{vol}/{modality_key}_ras_stitched.nii.gz'\n",
    "            \n",
    "            path = f'{n4_corrected_data_dir}/vol/{vol}/{modality_key}_ras_stitched.nii.gz'\n",
    "#             print(np.min(nb.load(path).get_fdata()), modality_key, vol)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('ERROR:', e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESCALING INTENSITIES OF STITCHED VOLUME ABOVE 0\n",
    "n4_dict = {}\n",
    "for vol in file_paths.keys():\n",
    "    try:\n",
    "        if vol_to_check_list is not None and vol not in vol_to_check_list:\n",
    "            continue\n",
    "        print(f'n4 processing part-2 started with {vol}...')\n",
    "        if vol_to_check_list is not None and vol not in vol_to_check_list:\n",
    "            continue\n",
    "        n4_dict[vol] = {}\n",
    "        in_stitched_file_path, in_stitched_img = file_paths[vol]['ONE']['IN'], read_ras(file_paths[vol]['ONE']['IN'])\n",
    "        n4_dict[vol]['N4_2'] = rescale(in_stitched_img, vol, in_stitched_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('ERROR:',e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_n4_process = True\n",
    "for vol in file_paths.keys():\n",
    "    try:\n",
    "        if vol_to_check_list is not None and vol not in vol_to_check_list:\n",
    "            continue\n",
    "        print(f'n4-biasfield-correction starting with {vol}...')\n",
    "        in_file = n4_dict[vol]['N4_2']['SCALED']\n",
    "        opp_file = file_paths[vol]['ONE']['OPP']\n",
    "        if all_n4_process:\n",
    "            w_file = file_paths[vol]['ONE']['W']\n",
    "            w_of = w_file.split('/')[-1].split('.')[0]\n",
    "            w_outputfile = f'{n4_corrected_data_dir}/vol/{vol}/{w_of}_n4_corrected_sitk.nii.gz'\n",
    "            SITK_N4_normalization(in_file, w_file, w_outputfile, do_casting=False)\n",
    "            n4_dict[vol]['N4_2']['W_CORRECTED'] = w_outputfile\n",
    "            \n",
    "            f_file = file_paths[vol]['ONE']['F']\n",
    "            f_of = f_file.split('/')[-1].split('.')[0]\n",
    "            f_outputfile = f'{n4_corrected_data_dir}/vol/{vol}/{f_of}_n4_corrected_sitk.nii.gz'\n",
    "            SITK_N4_normalization(in_file, f_file, f_outputfile, do_casting=False)\n",
    "            n4_dict[vol]['N4_2']['F_CORRECTED'] = f_outputfile\n",
    "            \n",
    "            inin_file = file_paths[vol]['ONE']['IN']\n",
    "            in_of = inin_file.split('/')[-1].split('.')[0]\n",
    "            in_outputfile = f'{n4_corrected_data_dir}/vol/{vol}/{in_of}_n4_corrected_sitk.nii.gz'\n",
    "            SITK_N4_normalization(in_file, inin_file, in_outputfile, do_casting=False)\n",
    "            n4_dict[vol]['N4_2']['IN_CORRECTED'] = in_outputfile\n",
    "            \n",
    "        new_filename = opp_file.split('/')[-1].split('.')[0]\n",
    "        output_file = f'{n4_corrected_data_dir}/vol/{vol}/{new_filename}_n4_corrected_sitk.nii.gz'\n",
    "        SITK_N4_normalization(in_file, opp_file, output_file, do_casting=False)\n",
    "        n4_dict[vol]['N4_2']['OPP_CORRECTED'] = output_file\n",
    "        file_paths[vol]['N4_2'] = n4_dict[vol]['N4_2']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('ERROR:',e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ukb_vol_label_fix(vol, label, use_alternate_approach=False):\n",
    "    world_shape = np.max(np.array([list(vol.shape), list(label.shape)]), axis=0)\n",
    "    final_label = np.zeros(tuple(world_shape))\n",
    "    \n",
    "    label_affine = label.affine\n",
    "    vol_affine = vol.affine\n",
    "    target_affine = vol_affine\n",
    "    target_header = vol.header\n",
    "\n",
    "    sx,sy,sz,ex,ey,ez = np.abs(get_points(label, vol))\n",
    "    labelmap = label.get_fdata()\n",
    "    \n",
    "    if not use_alternate_approach:\n",
    "        final_label[0:sx+ex, 0:sy+ey, sz:ez] = labelmap\n",
    "    else:\n",
    "        final_label[0:sx+ex, 0:sy+ey, sz-20:ez-20] = labelmap\n",
    "\n",
    "    final_label = np.flip(final_label, axis=0)\n",
    "    final_label = np.flip(final_label, axis=1)\n",
    "    \n",
    "    final_label_img = nb.Nifti1Image(final_label, target_affine, target_header)\n",
    "    \n",
    "    return vol, final_label_img\n",
    "\n",
    "def ukb_label_parts(label_parts, reference_labelmap=None):\n",
    "    stitched_label = None\n",
    "    mode = 'nearest'\n",
    "    order = 0\n",
    "    if reference_labelmap is None:\n",
    "        label_shape = np.max([img.shape for img, _, _ in label_parts], axis=0)\n",
    "        reference_labelmap = [img for img, _, _ in label_parts if list(img.shape) == list(label_shape)][0]\n",
    "    else:\n",
    "        label_shape = reference_labelmap.shape\n",
    "\n",
    "    stitched_label = np.zeros(label_shape)\n",
    "    for labelmap_img, lidx, lname in label_parts:\n",
    "        print(lidx, lname)\n",
    "        labelmap_img = makeit_3d(labelmap_img)\n",
    "        labelmap_img = resample_from_to(labelmap_img, [label_shape, reference_labelmap.affine], order=order, mode=mode, cval=0)\n",
    "        \n",
    "        sx,sy,sz,ex,ey,ez = np.abs(get_points(labelmap_img, reference_labelmap))\n",
    "        \n",
    "        labelmap = labelmap_img.get_fdata()\n",
    "        labelmap = np.multiply(lidx, labelmap)\n",
    "        stitched_label[0:ex+sx, 0:ey+sy, 0:ez+sz] += labelmap\n",
    "        \n",
    "        print(\"###############################################################################################\") \n",
    "        \n",
    "    labelmap = np.round(stitched_label)\n",
    "    stitched_labeled_img = nb.Nifti1Image(labelmap, reference_labelmap.affine, reference_labelmap.header)\n",
    "    \n",
    "    return stitched_labeled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ukb_vol_label_allignment(file_paths):\n",
    "    print(\"STARTING NAKO LABEL-MAPS.\")\n",
    "    print('Reading Label Maps.....')\n",
    "    for vol in file_paths.keys():\n",
    "        print(vol)\n",
    "        if vol_to_check_list is not None and vol not in vol_to_check_list or (vol == \"\") or (vol in exclude):\n",
    "            continue\n",
    "\n",
    "        print(file_paths[vol]['LABEL_PATHS'])\n",
    "        if len(file_paths[vol]['LABEL_PATHS']) == 0:\n",
    "            print(f\"#################### ALERT:: NO LABELPATHS IN THE DICTIONARY FOR {vol} #########################\")\n",
    "            continue\n",
    "\n",
    "        volume = nb.load(file_paths[vol]['N4_2']['OPP_CORRECTED'])\n",
    "        f_volume = nb.load(file_paths[vol]['N4_2']['F_CORRECTED'])\n",
    "        w_volume = nb.load(file_paths[vol]['N4_2']['W_CORRECTED'])\n",
    "        in_volume = nb.load(file_paths[vol]['N4_2']['IN_CORRECTED'])\n",
    "\n",
    "        img_ras_list = []\n",
    "        later = []\n",
    "        for label_file_to_read in file_paths[vol]['LABEL_PATHS']:\n",
    "            img_ras, lidx, labelname = read_ras(label_file_to_read, is_label=True)\n",
    "            if labelname is None or img_ras is None:\n",
    "                continue\n",
    "            img_ras = makeit_3d(img_ras)\n",
    "            if labelname in ['SPLEEN', 'PANCREAS']:\n",
    "                later.append([img_ras, lidx+LABEL_EXTENSION_FOR_OVERLAP_REMOVAL, labelname])\n",
    "            else:\n",
    "                img_ras_list.append([img_ras, lidx+LABEL_EXTENSION_FOR_OVERLAP_REMOVAL, labelname])\n",
    "\n",
    "        img_ras_list.extend(later)\n",
    "        s_label = ukb_label_parts(img_ras_list)\n",
    "        if SAMPLING:\n",
    "            s_label = resample_to_output(s_label, TARGET_RESOLUTION, order=0, mode='nearest', cval=0)\n",
    "            volume = resample_to_output(volume, TARGET_RESOLUTION, order=3, mode='constant', cval=0)\n",
    "            print(\"nnnn:  \", np.min(volume.get_fdata()))\n",
    "        s_label = drop_overlapped_pixels(s_label, np.array(img_ras_list)[:, 1])\n",
    "\n",
    "        if vol == '1004985_20201_2_0':\n",
    "            volume, s_label = ukb_vol_label_fix(volume, s_label, True)\n",
    "        else:\n",
    "            volume, s_label = ukb_vol_label_fix(volume, s_label)\n",
    "\n",
    "        print('Viewing Stitched Images.....')\n",
    "        volume_3_view_viewer(get_volume_data(volume))\n",
    "        volume_3_view_viewer(get_volume_data(f_volume))\n",
    "        volume_3_view_viewer(get_volume_data(w_volume))\n",
    "        volume_3_view_viewer(get_volume_data(in_volume))\n",
    "        volume_3_view_viewer(get_volume_data(s_label))\n",
    "\n",
    "        print('Saving Processed & Stitched Image.....')\n",
    "        save_volume(volume, f'{processed_dir}/volume/{vol}')\n",
    "        save_volume(f_volume, f'{processed_dir}/volume_f/{vol}')\n",
    "        save_volume(w_volume, f'{processed_dir}/volume_w/{vol}')\n",
    "        save_volume(in_volume, f'{processed_dir}/volume_in/{vol}')\n",
    "        save_volume(s_label, f'{processed_dir}/label/{vol}')\n",
    "        print('FINISHED.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_vol_label_allignment(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:remotenv]",
   "language": "python",
   "name": "conda-env-remotenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
