{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess abdominal MRI data from 3 datasets (KORA, NAKO, UKB) \n",
    "\n",
    "### preprocessing steps:\n",
    "1. combine scans for UKB -- separate .ipynb for that part. (since UKB is split into different scans for the abdomen. differs between scans, but most have 2 scans (upper and lower abdomen) that are relevant for us. combination with simple linear function in the overlap part. so far all scans have to be manually checked after the combination.\n",
    "2. combine segmentations --> see separate .ipynb for that part. (also handling different shapes of segmentations, different orientations, overlaps etc). Output of this step are image scans (input_scan.nii.gz and combined segmentation map: combined_segmentation.nii.gz, all in RAS orientation)\n",
    "3. this file: preprocessing 1: loads the datasets KORA, UKB or NAKO (or all 3), and resamples them to a common resolution of 2,2,3mm \n",
    "4. preprocessing 2: loads resampled files and normalizes the images by min max normalization. Then saves these images as nii files.\n",
    "\n",
    "Next step would be creating h5 files to train the quickNat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nibabel  # for loading and saving nifty files\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Pad\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nrrd\n",
    "from nibabel.affines import from_matvec, to_matvec, apply_affine\n",
    "\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labels:\n",
    "\n",
    "liver = 1  \n",
    "spleen = 2  \n",
    "kidney_r = 3  \n",
    "kidney_l = 4  \n",
    "adrenal_r = 5  \n",
    "adrenal_l = 6  \n",
    "pancreas = 7  \n",
    "gallbladder = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load datasets:\n",
    "\n",
    "directory structure should be like: .../datasets/dataID/data/patID/input_scan.nii.gz and   .../datasets/dataID/data/patID/combined_segmentation.nii.gz  \n",
    "    \n",
    "where dataID = KORA, NAKO or UKB  \n",
    "patIDs for training are in ../datasets/dataID/dataID.train  \n",
    "e.g. for KORA: .../datasets/KORA/KORA.train  \n",
    "\n",
    "this has to be run separately for train, test and val split that are indicated in dataID.train, dataID.val, dataID.test files  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2456289']\n",
      "segm max  9.0\n",
      "segm_test max  9\n",
      "{'KORA': ['2456289'], 'NAKO': [], 'UKB': []}\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/anne/phd/projects/whole_body/whole_body_segmentation/quickNAT_pytorch/create_datasets/data/new/KORA/missing'\n",
    "all_images_train = {'KORA': [],\n",
    "             'NAKO': [],\n",
    "             'UKB' : []}\n",
    "all_pat_ids = {'KORA': [],\n",
    "             'NAKO': [],\n",
    "             'UKB' : []}\n",
    "for i, data_id in enumerate(['KORA']):\n",
    "    data_dir = os.path.join(root_dir, data_id, 'data')\n",
    "    filenames = os.path.join(root_dir, data_id, data_id+'.train')\n",
    "    # read filenames\n",
    "    with open(filenames) as f:\n",
    "        pat_ids = f.read().splitlines()   \n",
    "    all_pat_ids[data_id] = pat_ids\n",
    "\n",
    "    print(pat_ids)\n",
    "    for folder in pat_ids:\n",
    "        seg_dir = os.path.join(data_dir,folder)\n",
    "        #listFiles = os.listdir(seg_dir)\n",
    "        #seg_files = [f for f in listFiles if \"input_scan\" in f or 'combined_segmentation' in f]\n",
    "        try:\n",
    "            img = nibabel.load(os.path.join(seg_dir,'input_scan.nii.gz'))\n",
    "            segm = nibabel.load(os.path.join(seg_dir,'combined_segmentation.nii.gz'))\n",
    "            fat = nibabel.load(os.path.join(seg_dir,'fat.nii.gz'))\n",
    "            water = nibabel.load(os.path.join(seg_dir,'water.nii.gz'))\n",
    "            \n",
    "            #print(segm.get_fdata().dtype)\n",
    "            segm_test = segm.get_fdata().astype('int')\n",
    "            print('segm max ', np.max(segm.get_fdata()))\n",
    "            print('segm_test max ', np.max(segm_test))\n",
    "            #print(segm_test.dtype)\n",
    "        except FileNotFoundError as err:\n",
    "            print('did not find input scan AND segmentation for patient id: ', folder)\n",
    "            print(err)\n",
    "        \n",
    "        all_images_train[data_id].append((img,fat,water,segm))\n",
    "#print(all_images_train)\n",
    "print(all_pat_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing 1 (resampling) can be skipped. if already resampled proceed with preprocessing 2\n",
    "\n",
    "first checks if segmentations have no overlaps  \n",
    "if overlaps are found than this needs to be handled first  \n",
    "then resamples the images to 2,2,3mm resolution --> takes a while\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KORA\n",
      "0.0\n",
      "8.0\n",
      "no overlaps in segmentation\n",
      "min intensity:  0.0\n",
      "max intensity:  982.0\n",
      "steps:  [1.7013888 1.7013888 1.699997 ]\n",
      "32249\n",
      "correcting fat and water images\n",
      "RAS transformed to RAS\n",
      "RAS transformed to RAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anne/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: get_affine method is deprecated.\n",
      "Please use the ``img.affine`` property instead.\n",
      "\n",
      "* deprecated from version: 2.1\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13238\n",
      "13238\n",
      "shape before:  (245, 149, 163)\n",
      "shape after:  (256, 256, 128)\n",
      "AFFINE:  [[   2.            0.            0.         -235.26948547]\n",
      " [   0.            2.            0.           21.65872765]\n",
      " [   0.            0.            3.         -144.93167114]\n",
      " [   0.            0.            0.            1.        ]]\n",
      "NAKO\n",
      "UKB\n"
     ]
    }
   ],
   "source": [
    "for i, data_id in enumerate(['KORA', 'NAKO', 'UKB']):\n",
    "    print(data_id)\n",
    "    images = all_images_train[data_id]\n",
    "\n",
    "    #ax = plt.hist(images.ravel(), bins = 256)\n",
    "    #plt.show()\n",
    "    for j, imgs in enumerate(images):\n",
    "        img_data = imgs[0].get_fdata()\n",
    "        img_header = imgs[0].header\n",
    "        fat = imgs[1]\n",
    "        water = imgs[2]\n",
    "        segm_data = imgs[3].get_fdata()\n",
    "        segm_header = imgs[3].header\n",
    "        \n",
    "        print(np.min(segm_data))\n",
    "        print(np.max(segm_data))\n",
    "        if np.max(segm_data) > 8:\n",
    "            print('max bigger than 8 for: ', data_id, all_pat_ids[data_id][j] )\n",
    "            print ('max: ', np.max(segm_data))    \n",
    "            segm_data[segm_data==9.0] = 7.0\n",
    "            print ('max: ', np.max(segm_data)) \n",
    "            print(np.where(segm_data==np.max(segm_data)))\n",
    "        else:\n",
    "            print('no overlaps in segmentation')\n",
    "        \n",
    "        print('min intensity: ', np.min(img_data))\n",
    "        print('max intensity: ', np.max(img_data))\n",
    "        \n",
    "        steps = img_header['pixdim'][1:4]\n",
    "        target_voxel_dim = [2,2,3]\n",
    "        print('steps: ', steps)\n",
    "        \n",
    "        segm_data = segm_data.astype('int')\n",
    "        \n",
    "        print(np.sum(segm_data == 4))\n",
    "        \n",
    "        print('correcting fat and water images')\n",
    "        fat_img, fat_data, fat_header = reorient_image(fat, is_label=False)\n",
    "        water_img, water_data, water_header = reorient_image(water, is_label=False)\n",
    "\n",
    "\n",
    "        if data_id == 'KORA':\n",
    "            img_data = np.flip(np.moveaxis(img_data, 2, 1))  # 012 -> 021\n",
    "            fat_data = np.flip(np.moveaxis(fat_data, 2, 1))  # 012 -> 021\n",
    "            water_data = np.flip(np.moveaxis(water_data, 2, 1))  # 012 -> 021\n",
    "            segm_data = np.flip(np.moveaxis(segm_data, 2, 1))  # 012 -> 021\n",
    "            \n",
    "        img_data, img_header = do_interpolate(img_data, steps, img_header, target_voxel_dim)\n",
    "        fat_data, fat_header = do_interpolate(fat_data, steps, fat_header, target_voxel_dim)\n",
    "        water_data, water_header = do_interpolate(water_data, steps, water_header, target_voxel_dim)\n",
    "        segm_data, segm_header = do_interpolate(segm_data, steps, segm_header, target_voxel_dim, is_label=True)\n",
    "        \n",
    "        print(np.sum(segm_data == 4))\n",
    "        #print(np.unique(segm_data))\n",
    "        \n",
    "        segm_data = segm_data.astype('int')\n",
    "        print(np.sum(segm_data == 4))\n",
    "\n",
    "        \n",
    "        print('shape before: ', img_data.shape)\n",
    "        # now reshape the input scans to a target dimension dim % 16 = 0 so all images have the same shape\n",
    "        target_dim = [256,256,128]\n",
    "\n",
    "        img_data, fat_data,water_data, segm_data = post_interpolate(img_data, fat_data,water_data,labelmap=segm_data, target_shape=target_dim)\n",
    "        \n",
    "        print('shape after: ', img_data.shape)\n",
    "        img_header['dim'][1:4] = img_data.shape\n",
    "        segm_header['dim'][1:4] = segm_data.shape\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        print('AFFINE: ', img_header.get_best_affine())\n",
    "        #save images:\n",
    "        \n",
    "        #new_img_nii = nibabel.Nifti1Image(img_data, img_header.get_best_affine(), img_header)\n",
    "        new_img_nii = nibabel.MGHImage(img_data, img_header.get_best_affine(), img_header)\n",
    "        new_file_path = os.path.join(root_dir,data_id,'data', all_pat_ids[data_id][j],'resampled_image.nii.gz')\n",
    "        nibabel.save(new_img_nii, new_file_path)\n",
    "        \n",
    "        new_img_nii = nibabel.MGHImage(fat_data, fat_header.get_best_affine(), fat_header)\n",
    "        new_file_path = os.path.join(root_dir,data_id,'data', all_pat_ids[data_id][j],'resampled_fat.nii.gz')\n",
    "        nibabel.save(new_img_nii, new_file_path)\n",
    "        \n",
    "        new_img_nii = nibabel.MGHImage(water_data, water_header.get_best_affine(), water_header)\n",
    "        new_file_path = os.path.join(root_dir,data_id,'data', all_pat_ids[data_id][j],'resampled_water.nii.gz')\n",
    "        nibabel.save(new_img_nii, new_file_path)\n",
    "        \n",
    "        #new_img_nii = nibabel.Nifti1Image(segm_data, segm_header.get_best_affine(), segm_header)\n",
    "        new_img_nii = nibabel.MGHImage(segm_data, segm_header.get_best_affine(), segm_header)\n",
    "        new_img_nii.set_data_dtype('uint8')\n",
    "        new_file_path = os.path.join(root_dir,data_id,'data', all_pat_ids[data_id][j],'resampled_segm.nii.gz')\n",
    "        nibabel.save(new_img_nii, new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing 2\n",
    "\n",
    "scale the pixel values between 0 and 1 (min max normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data again (this time load the resampled scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2456289']\n",
      "{'KORA': ['2456289'], 'NAKO': [], 'UKB': []}\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/anne/phd/projects/whole_body/whole_body_segmentation/quickNAT_pytorch/create_datasets/data/new/KORA/missing'\n",
    "all_images_train = {'KORA': [],\n",
    "             'NAKO': [],\n",
    "             'UKB' : []}\n",
    "all_pat_ids = {'KORA': [],\n",
    "             'NAKO': [],\n",
    "             'UKB' : []}\n",
    "for i, data_id in enumerate(['KORA']):\n",
    "    data_dir = os.path.join(root_dir, data_id, 'data')\n",
    "    filenames = os.path.join(root_dir, data_id, data_id+'.train')\n",
    "    # read filenames\n",
    "    with open(filenames) as f:\n",
    "        pat_ids = f.read().splitlines()   \n",
    "    all_pat_ids[data_id] = pat_ids\n",
    "\n",
    "    print(pat_ids)\n",
    "    for folder in pat_ids:\n",
    "        seg_dir = os.path.join(data_dir,folder)\n",
    "        #listFiles = os.listdir(seg_dir)\n",
    "        #seg_files = [f for f in listFiles if \"input_scan\" in f or 'combined_segmentation' in f]\n",
    "        try:\n",
    "            img = nibabel.load(os.path.join(seg_dir,'resampled_image.nii.gz'))\n",
    "            fat = nibabel.load(os.path.join(seg_dir,'resampled_fat.nii.gz'))\n",
    "            water = nibabel.load(os.path.join(seg_dir,'resampled_water.nii.gz'))\n",
    "            segm = nibabel.load(os.path.join(seg_dir,'resampled_segm.nii.gz'))\n",
    "        except FileNotFoundError as err:\n",
    "            print('did not find input scan AND segmentation for patient id: ', folder)\n",
    "            print(err)\n",
    "        \n",
    "        all_images_train[data_id].append((img,fat,water,segm))\n",
    "#print(all_images_train)\n",
    "print(all_pat_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min max normalization for each dataset separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KORA\n",
      "NAKO\n",
      "UKB\n"
     ]
    }
   ],
   "source": [
    "for i, data_id in enumerate(['KORA', 'NAKO', 'UKB']):\n",
    "    print(data_id)\n",
    "    images = all_images_train[data_id]\n",
    "    \n",
    "    for j, imgs in enumerate(images):\n",
    "        img = imgs[0].get_fdata()\n",
    "        img_header = imgs[0].header\n",
    "        \n",
    "        fat = imgs[1].get_fdata()\n",
    "        fat_header = imgs[1].header\n",
    "        \n",
    "        water = imgs[2].get_fdata()\n",
    "        water_header = imgs[2].header\n",
    "        \n",
    "        fat = (fat - np.min(fat))/(np.max(fat)-np.min(fat))\n",
    "        water = (water - np.min(water))/(np.max(water)-np.min(water))\n",
    "        img_min = np.min(img)\n",
    "        img_max = np.max(img)\n",
    "        img = (img - img_min)/(img_max -img_min)\n",
    "                \n",
    "        new_img_nii = nibabel.Nifti1Image(img, img_header.get_best_affine(), img_header)        \n",
    "        new_file_path = os.path.join(root_dir,data_id,'data', all_pat_ids[data_id][j],'resampled_normalized_image.nii.gz')\n",
    "        nibabel.save(new_img_nii, new_file_path)\n",
    "        \n",
    "        new_img_nii = nibabel.Nifti1Image(fat, fat_header.get_best_affine(), fat_header)        \n",
    "        new_file_path = os.path.join(root_dir,data_id,'data', all_pat_ids[data_id][j],'resampled_normalized_fat.nii.gz')\n",
    "        nibabel.save(new_img_nii, new_file_path)\n",
    "        \n",
    "        new_img_nii = nibabel.Nifti1Image(water, water_header.get_best_affine(), water_header)        \n",
    "        new_file_path = os.path.join(root_dir,data_id,'data', all_pat_ids[data_id][j],'resampled_normalized_water.nii.gz')\n",
    "        nibabel.save(new_img_nii, new_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as si\n",
    "\n",
    "def do_interpolate(source, steps, header, target_voxel_dim, is_label=False):\n",
    "    x, y, z = [steps[k] * np.arange(source.shape[k]) for k in range(3)]\n",
    "    #print('x ', x)\n",
    "    #print('y ', y)\n",
    "    #print('z ', z)\n",
    "    \n",
    "    if is_label:\n",
    "        method = 'nearest'\n",
    "    else:\n",
    "        method = 'linear'\n",
    "\n",
    "    f = si.RegularGridInterpolator((x, y, z), source, method=method)\n",
    "\n",
    "    dx, dy, dz = target_voxel_dim\n",
    "    affine = header.get_best_affine()\n",
    "    np.fill_diagonal(affine, target_voxel_dim + [1])\n",
    "    header.set_sform(affine)\n",
    "    new_grid = np.mgrid[0:x[-1]:dx, 0:y[-1]:dy, 0:z[-1]:dz]\n",
    "    new_grid = np.moveaxis(new_grid, (0, 1, 2, 3), (3, 0, 1, 2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    interpolated = f(new_grid)\n",
    "    \n",
    "   \n",
    "    if is_label:\n",
    "        #print(np.unique(interpolated))\n",
    "        #print(np.mean(interpolated))\n",
    "        #print(np.max(interpolated))\n",
    "        #print('3s ', np.sum(interpolated ==3.98))\n",
    "        \n",
    "        #print('kidney l ', interpolated[56,68,34])\n",
    "        #print(np.where(interpolated==4))\n",
    "        interpolated = np.round(interpolated)\n",
    "        #print('rounded')\n",
    "        #print('kidney l ', interpolated[56,68,34])\n",
    "\n",
    "        #print('3s ', np.sum(interpolated ==3.98))\n",
    "        #interpolated = interpolated.astype('int')\n",
    "        #print('3s ', np.sum(interpolated ==3))\n",
    "\n",
    "    return interpolated, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(source_num_arr, target_array=np.arange(0, 500, 16)):\n",
    "    array = np.asarray(target_array)\n",
    "    idxs = [(np.abs(array - source_num)).argmin() for source_num in source_num_arr]\n",
    "    return [array[idx + 1] for idx in idxs]\n",
    "\n",
    "\n",
    "def post_interpolate(volume,fat,water, labelmap=None, target_shape=None):\n",
    "    volume = do_cropping(volume, target_shape)\n",
    "    fat = do_cropping(fat, target_shape)\n",
    "    water = do_cropping(water,target_shape)\n",
    "    if labelmap is not None:\n",
    "        labelmap = do_cropping(labelmap, target_shape)\n",
    "    current_shape = volume.shape\n",
    "    intended_shape_deficit = target_shape - np.asarray(current_shape)\n",
    "\n",
    "    paddings = [tuple(\n",
    "        np.array([np.ceil((pad_tuples / 2) - pad_tuples % 2), np.floor((pad_tuples / 2) + pad_tuples % 2)]).astype(\n",
    "            'int32')) for pad_tuples in intended_shape_deficit]\n",
    "    paddings = tuple(paddings)\n",
    "\n",
    "    volume = np.pad(volume, paddings, mode='constant')\n",
    "    fat = np.pad(fat, paddings, mode='constant')\n",
    "    water = np.pad(water,paddings,mode='constant')\n",
    "    if labelmap is not None:\n",
    "        labelmap = np.pad(labelmap, paddings, mode='constant')\n",
    "\n",
    "    return volume,fat, water, labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def do_cropping(source_num_arr, bounding):\n",
    "    start = list(map(lambda a, da: a // 2 - da // 2, source_num_arr.shape, bounding))\n",
    "    end = list(map(operator.add, start, bounding))\n",
    "    for i, val in enumerate(zip(start, end)):\n",
    "        if val[0] < 0:\n",
    "            start[i] = 0\n",
    "            end[i] = source_num_arr.shape[i]\n",
    "    slices = tuple(map(slice, tuple(start), tuple(end)))\n",
    "    return source_num_arr[slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorient_image(image, axes='RAS', translation_params=(0, 0, 0), is_label=False):\n",
    "    # Global parameters\n",
    "    POSSIBLE_AXES_ORIENTATIONS = [\n",
    "        \"LAI\", \"LIA\", \"ALI\", \"AIL\", \"ILA\", \"IAL\",\n",
    "        \"LAS\", \"LSA\", \"ALS\", \"ASL\", \"SLA\", \"SAL\",\n",
    "        \"LPI\", \"LIP\", \"PLI\", \"PIL\", \"ILP\", \"IPL\",\n",
    "        \"LPS\", \"LSP\", \"PLS\", \"PSL\", \"SLP\", \"SPL\",\n",
    "        \"RAI\", \"RIA\", \"ARI\", \"AIR\", \"IRA\", \"IAR\",\n",
    "        \"RAS\", \"RSA\", \"ARS\", \"ASR\", \"SRA\", \"SAR\",\n",
    "        \"RPI\", \"RIP\", \"PRI\", \"PIR\", \"IRP\", \"IPR\",\n",
    "        \"RPS\", \"RSP\", \"PRS\", \"PSR\", \"SRP\", \"SPR\"\n",
    "    ]\n",
    "\n",
    "  \n",
    "\n",
    "    header = image.header\n",
    "    #print(header)    \n",
    "    axes = nibabel.orientations.aff2axcodes(header.get_best_affine())\n",
    "    axes = \"\".join(list(axes))\n",
    "\n",
    "    # Check that a valid input axes is specified\n",
    "    if axes not in POSSIBLE_AXES_ORIENTATIONS:\n",
    "        raise ValueError(\"Wrong coordinate system: {0}.\".format(axes))\n",
    "\n",
    "    rotation = swap_affine(axes)\n",
    "    det = np.linalg.det(rotation)\n",
    "    if det != 1:\n",
    "        raise Exception(\"Rotation matrix determinant must be one \"\n",
    "                        \"not '{0}'.\".format(det))\n",
    "\n",
    "    affine = image.get_affine()\n",
    "    # Get the trnaslation to apply\n",
    "    translation = np.eye(4)\n",
    "    # Get the input image affine transform\n",
    "    # s, t = to_matvec(affine)\n",
    "    # print(affine, s, t, tuple(t))\n",
    "    # print(header.get_best_affine())\n",
    "    # volume_translations = [-216, -178.9, -664.5]\n",
    "    # current_t = [-119, -106, -503]\n",
    "    # intended_t = tuple(np.subtract(volume_translations, t))\n",
    "    # print(intended_t)\n",
    "    translation[:3, 3] = translation_params  # tuple(t) #if not is_label else translation_params[:3, 3]\n",
    "    # print(affine, data.shape)\n",
    "\n",
    "    # source_orientation = nb.orientations.io_orientation(affine)\n",
    "    # transformation_mat = nb.orientations.ornt_transform(source_orientation, PreProcess.get_target_orientation())\n",
    "    # data = nb.orientations.apply_orientation(data, transformation_mat)\n",
    "    # print(data.shape)\n",
    "\n",
    "    # Apply the rotation to set the image in the RAS coordiante system\n",
    "    transformation = np.dot(np.dot(rotation, affine), translation)\n",
    "    # print('transformation mat:', transformation)\n",
    "    # image.set_qform(transformation)\n",
    "    image.set_sform(transformation)\n",
    "\n",
    "\n",
    "    data = image.get_data()\n",
    "    # print('after transformation:', data.shape)\n",
    "    # affine = image.get_affine()\n",
    "    # s, t = to_matvec(affine)\n",
    "    # print(affine, s, t, tuple(t))\n",
    "    # translation = np.eye(4)\n",
    "    # translation[:3, 3] = tuple(-t)\n",
    "    # image.set_sform(translation)\n",
    "    # data = image.get_data()\n",
    "    # print('after translation:', data.shape)\n",
    "\n",
    "    # array = image.get_data()\n",
    "    # print(image.get_affine(), array.shape)\n",
    "\n",
    "    header = image.header\n",
    "    new_axes = nibabel.orientations.aff2axcodes(header.get_best_affine())\n",
    "    new_axes = \"\".join(list(new_axes))\n",
    "    # 216.09375, 178.90625, -664.5\n",
    "    print(f\"{axes} transformed to {new_axes}\")\n",
    "    # print(data.shape, header.get_best_affine())\n",
    "    # print('-' * 39)\n",
    "    # if not is_label:\n",
    "    #     return image, data, header, translation\n",
    "    return image, data, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_affine(axes):\n",
    "    CORRECTION_MATRIX_COLUMNS = {\n",
    "        \"R\": (1, 0, 0),\n",
    "        \"L\": (-1, 0, 0),\n",
    "        \"A\": (0, 1, 0),\n",
    "        \"P\": (0, -1, 0),\n",
    "        \"S\": (0, 0, 1),\n",
    "        \"I\": (0, 0, -1)\n",
    "    }\n",
    "\n",
    "    if axes not in ['RSP', 'LIP', 'RAS', 'LPS']:\n",
    "        raise Exception(\n",
    "            f'Unknown axes passed for affine transformation! Please add transformation for {axes} manually.')\n",
    "    rotation = np.eye(4)\n",
    "    rotation[:3, 0] = CORRECTION_MATRIX_COLUMNS[axes[0]]\n",
    "    rotation[:3, 1] = CORRECTION_MATRIX_COLUMNS[axes[1]]\n",
    "    rotation[:3, 2] = CORRECTION_MATRIX_COLUMNS[axes[2]]\n",
    "    # print(rotation)\n",
    "    if axes == \"RSP\":\n",
    "        rotation = np.array([[1., 0., 0., 0.],\n",
    "                             [0., 0., 1., 0.],\n",
    "                             [0., -1., 0., 0.],\n",
    "                             [0., 0., 0., 1.]])\n",
    "        # print(rotation)\n",
    "    elif axes == \"LPS\":\n",
    "        rotation = np.array([\n",
    "            [-1., 0., 0., 0.],\n",
    "            [0., -1., 0., 0.],\n",
    "            [0., 0., 1., 0.],\n",
    "            [0., 0., 0., 1.]\n",
    "        ])\n",
    "        # print(rotation)\n",
    "    return rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
